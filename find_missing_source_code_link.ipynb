{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to find missing GH repos for advisories that are missing\n",
    "\n",
    "* Requires you to run ecosystem_identify_missing_vfc_adv.ipynb\n",
    "    * Generates: ./data/final_data/advisories_missing_GH_repo_20221204.csv\n",
    "    * Prior cell to last cell in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "from xml.etree import ElementTree\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to load current GHSA DB\n",
    "\n",
    "* Make sure you have the GHSA DB locally cloned\n",
    "    * set the ghsa_db_path to your cloned location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghsa_db_path = f\"../advisory-database/advisories/github-reviewed/\"\n",
    "\n",
    "# get all JSON files\n",
    "ghsa_files = glob.glob(f\"{ghsa_db_path}*/*/*/*.json\")\n",
    "\n",
    "df_ghsa = pd.DataFrame()\n",
    "\n",
    "# load all json files to obtain the \"PACKAGE\" within the references list\n",
    "for index, file in enumerate(ghsa_files):\n",
    "    # placeholders for GHSA Info\n",
    "    id = None\n",
    "    package_ecosystem = None\n",
    "    package_name = None\n",
    "    references_package = None\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        # load JSON\n",
    "        temp_file = json.load(f)\n",
    "        \n",
    "        # set GHSA info\n",
    "        id = temp_file['id']\n",
    "        package_ecosystem = temp_file[\"affected\"][0][\"package\"][\"ecosystem\"]\n",
    "        package_name = temp_file[\"affected\"][0][\"package\"][\"name\"]\n",
    "        \n",
    "        # check each reference for the package (source code) url\n",
    "        for temp_ref in temp_file[\"references\"]:\n",
    "            if temp_ref[\"type\"] == \"PACKAGE\":\n",
    "                references_package = temp_ref[\"url\"]\n",
    "        \n",
    "        # append to df_ghsa\n",
    "        df_ghsa = pd.concat([df_ghsa, pd.DataFrame([[id, package_ecosystem,\n",
    "                                                     package_name, references_package]],\n",
    "                                                   columns=[\"id\", \"package_ecosystem\",\n",
    "                                                            \"package_name\", \"references_package\"])])\n",
    "    \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghsa.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for advisories with missing source code link\n",
    "ghsa_missing_GH_repo = df_ghsa[df_ghsa['references_package'].isna()]\n",
    "\n",
    "print(f\"Unique GHSA Advisories loaded: {df_ghsa.id.nunique()}\")\n",
    "print(f\"GHSA without a package (source code) link: {ghsa_missing_GH_repo.id.nunique()}\\n\")\n",
    "\n",
    "# create a breakdown of ecosystem info\n",
    "breakdowns = df_ghsa.package_ecosystem.value_counts().to_frame().reset_index(drop=False)\n",
    "breakdowns.columns = [\"ecosystem\", \"ghsa_count\"]\n",
    "breakdowns_missing = ghsa_missing_GH_repo.package_ecosystem.value_counts().to_frame().reset_index(drop=False)\n",
    "breakdowns_missing.columns = [\"ecosystem\", \"source_link_missing_count\"]\n",
    "\n",
    "breakdowns = pd.merge(breakdowns, breakdowns_missing,\n",
    "                      on=[\"ecosystem\"],\n",
    "                      how=\"left\")\n",
    "\n",
    "breakdowns = breakdowns.fillna(0)\n",
    "\n",
    "breakdowns[\"percent_missing_source_link\"] = breakdowns.apply(\n",
    "      lambda x: f\"{round(100*x['source_link_missing_count']/x['ghsa_count'], 2)}%\",\n",
    "      axis=1\n",
    ")\n",
    "\n",
    "breakdowns[\"missing_complete\"] = breakdowns.apply(\n",
    "      lambda x: f\"{x['source_link_missing_count']} ({x['percent_missing_source_link']})\",\n",
    "      axis=1\n",
    ")\n",
    "\n",
    "breakdowns.head(n=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAVEN Finds\n",
    "\n",
    "* API Guide from SonaType\n",
    "    * https://central.sonatype.org/search/rest-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maven_pom_scm_check(groupId, artifactId, latest_version):\n",
    "    \"\"\"Check for SCM in Maven POM file\n",
    "\n",
    "    Args:\n",
    "        groupId (_type_): _description_\n",
    "        artifactId (_type_): _description_\n",
    "        latest_version (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # generate pom xml filepath from Maven search API\n",
    "    url = f\"https://search.maven.org/remotecontent?filepath={'/'.join(groupId.split('.'))}/{artifactId}/{latest_version}/{artifactId}-{latest_version}.pom\"\n",
    "    response = requests.get(url)\n",
    "    # response.close()\n",
    "    \n",
    "    # holder value for SCM repo\n",
    "    scm_repo = None\n",
    "    \n",
    "    try:\n",
    "        # parse the XML response\n",
    "        tree = ElementTree.fromstring(response.content)\n",
    "                \n",
    "        # iterate through children in tree\n",
    "        for child in tree.findall('*'):\n",
    "            # if SCM appear then set the scm_repo\n",
    "            if 'scm' in child.tag:\n",
    "                # find the child tag\n",
    "                scm_tags = tree.findall(child.tag)\n",
    "                # for each child tag\n",
    "                for scm_tags_repo in scm_tags:\n",
    "                    # find all repos in scm child tag\n",
    "                    scm_repos = scm_tags_repo.findall('*')\n",
    "                    # pull the text tag for the scm tag\n",
    "                    for repos in scm_repos:\n",
    "                        scm_repo = repos.text\n",
    "                        \n",
    "        response.close()\n",
    "        return scm_repo   \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failure in file request: {url} | {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maven_search(package_name, id):\n",
    "    \"\"\"Search for the Maven project name to see if it exists\n",
    "\n",
    "    Args:\n",
    "        package_name (str): Target package\n",
    "\n",
    "    Returns:\n",
    "        str: Repo link\n",
    "    \"\"\"\n",
    "    # parse groupId/artifactId from package name\n",
    "    groupId = package_name.split(':')[0]\n",
    "    artifactId = package_name.split(':')[1]\n",
    "    \n",
    "    # set url\n",
    "    url = f\"https://search.maven.org/solrsearch/select?q={groupId}+AND+a:{artifactId}&rows=10&wt=json\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.close()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        temp = response.json()\n",
    "        if temp[\"response\"][\"numFound\"] >= 1:\n",
    "            # print(temp[\"response\"][\"numFound\"])\n",
    "            # obtain the latestVersion so we can obtain the pom.xml file\n",
    "            for temp_response in temp['response']['docs']:\n",
    "                # make sure the id matches the package_name for the search\n",
    "                if temp_response['id'] == package_name:\n",
    "                    latestVersion = temp_response['latestVersion']\n",
    "    \n",
    "                    # get pom.xml file\n",
    "                    temp_scm_repo = maven_pom_scm_check(groupId, artifactId, latestVersion)\n",
    "                    \n",
    "                    # print(f\"{package_name} | repo={temp_scm_repo}\")\n",
    "                    \n",
    "                    return temp_scm_repo\n",
    "        else:\n",
    "            print(f\"{package_name} | 0 matches in search\")\n",
    "    else:\n",
    "        print(f\"{url} | Non-200 response\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maven_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"Maven\"]\n",
    "\n",
    "maven_missing = maven_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing Maven: {maven_missing.id.nunique()}\")\n",
    "\n",
    "maven_missing['temp_repo'] = maven_missing.apply(\n",
    "    lambda x: maven_search(x['package_name'], x['id']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if github is in link\n",
    "maven_missing[\"github_repo\"] = maven_missing.apply(\n",
    "    lambda x: x['temp_repo'] if 'github.com' in str(x['temp_repo']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# cleaning the scm links\n",
    "maven_missing[\"github_repo\"] = maven_missing.apply(\n",
    "    lambda x: f\"https://github.com/{x['github_repo'].split('@github.com:')[-1]}\" if '@github.com:' in str(x['github_repo']) else x['github_repo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# remove .git from link\n",
    "maven_missing[\"github_repo\"] = maven_missing.apply(\n",
    "    lambda x: x['github_repo'].replace('.git', '') if x['github_repo'] != None else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Found GH Repo: {maven_missing[maven_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo: {maven_missing[maven_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "maven_missing_final = maven_missing[[\"id\",\"package_ecosystem\",\n",
    "                                 \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "maven_missing_final = maven_missing_final[maven_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {maven_missing_final.id.nunique()} Maven GHSAs | ./missing_GH_info/maven_20230111.csv\")\n",
    "\n",
    "maven_missing_final.to_csv(f\"./missing_GH_info/maven_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPM Finds\n",
    "\n",
    "* We can directly hit the NPM registry at https://registry.npmjs.org/{package_name}\n",
    "    * The response is a JSON that we can parse for the repository url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npm_registry_source_code(package_name: str) -> str:\n",
    "    \"\"\"Obtain the repository git link\n",
    "\n",
    "    Args:\n",
    "        package_name (str): Target package name\n",
    "\n",
    "    Returns:\n",
    "        str: Git repo link\n",
    "    \"\"\"\n",
    "    # set URL\n",
    "    url = f\"https://registry.npmjs.org/{package_name}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.close()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        temp = response.json()\n",
    "        try:\n",
    "            repo = temp['repository']['url']\n",
    "            repo_match = re.findall(\"(?=github.com\\/)(.*)(\\/)(.*)\", repo.strip('.git'))\n",
    "            if len(repo_match) > 0:\n",
    "                repo_clean = ''.join(repo_match[0])\n",
    "                repo_clean = f\"https://{repo_clean}\"\n",
    "                return repo_clean\n",
    "        except Exception as e:\n",
    "            # print(f\"{url} | {str(e)}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"{url} | Non-200 response\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npm_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"npm\"]\n",
    "\n",
    "npm_missing = npm_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing NPM: {npm_missing.id.nunique()}\")\n",
    "\n",
    "npm_missing['temp_repo'] = npm_missing.apply(\n",
    "    lambda x: npm_registry_source_code(x['package_name']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with NPM, malicious packages have been removed and replaced with \"https://github.com/npm/security-holder\"\n",
    "print(f\"Setting {npm_missing[npm_missing['temp_repo']=='https://github.com/npm/security-holder'].id.nunique()}\"\n",
    "      f\" GHSAs to None due to security-holder from NPM\\n\")\n",
    "\n",
    "npm_missing[\"github_repo\"] = npm_missing.apply(\n",
    "    lambda x: None if x['temp_repo'] == \"https://github.com/npm/security-holder\" else x['temp_repo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Found GH Repo for NPM GHSAs: {npm_missing[npm_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for NPM GHSAs: {npm_missing[npm_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "npm_missing_final = npm_missing[[\"id\",\"package_ecosystem\",\n",
    "                                 \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "npm_missing_final = npm_missing_final[npm_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {npm_missing_final.id.nunique()} NPM GHSAs | ./missing_GH_info/npm_20230111.csv\")\n",
    "\n",
    "npm_missing_final.to_csv(f\"./missing_GH_info/npm_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPI Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url=url)  # request response from url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "class GetPackageProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.home_page = None\n",
    "        self.bug_tracker = None\n",
    "        self.documentation = None\n",
    "        self.source_code = None\n",
    "        self.url = f\"https://pypi.org/project/{self.package_name}/\"\n",
    "\n",
    "        soup = get_soup(self.url)\n",
    "        table = soup.find_all(\"div\", attrs={\"class\": \"sidebar-section\"})\n",
    "\n",
    "        for each in table:\n",
    "            for row in each.find_all(\"a\"):\n",
    "                if row.text.strip() == \"Homepage\":\n",
    "                    self.home_page = row.get(\"href\")\n",
    "                if row.text.strip() == \"Bug Tracker\":\n",
    "                    self.bug_tracker = row.get(\"href\")\n",
    "                if row.text.strip() == \"Documentation\":\n",
    "                    self.documentation = row.get(\"href\")\n",
    "                if row.text.strip() == \"Source\":\n",
    "                    self.source_code = row.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypi_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"PyPI\"]\n",
    "\n",
    "pypi_missing = pypi_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing PyPI: {pypi_missing.id.nunique()}\")\n",
    "\n",
    "# get all potential links\n",
    "pypi_missing[\"links\"] = pypi_missing.apply(\n",
    "    lambda x: GetPackageProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "pypi_missing[\"source_code\"] = pypi_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set home page link\n",
    "pypi_missing[\"home_page\"] = pypi_missing.apply(\n",
    "    lambda x: x['links'].home_page,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "pypi_missing[\"github_repo\"] = pypi_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else None,\n",
    "    axis=1\n",
    ")\n",
    "# check home_page for github link\n",
    "pypi_missing[\"github_repo\"] = pypi_missing.apply(\n",
    "    lambda x: x['home_page'] if 'github.com/' in str(x['home_page']) else x['github_repo'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for PyPI GHSAs: {pypi_missing[pypi_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for PyPI GHSAs: {pypi_missing[pypi_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "pypi_missing_final = pypi_missing[[\"id\",\"package_ecosystem\",\n",
    "                                 \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "pypi_missing_final = pypi_missing_final[pypi_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {pypi_missing_final.id.nunique()} PyPI GHSAs | ./missing_GH_info/pypi_20230111.csv\")\n",
    "\n",
    "pypi_missing_final.to_csv(f\"./missing_GH_info/pypi_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packagist Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url=url)  # request response from url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "class PackagistProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.canonical_repo = None\n",
    "        self.home_page = None\n",
    "        self.source_code = None\n",
    "        self.issues = None\n",
    "        self.url = f\"https://packagist.org/packages/{self.package_name}\"\n",
    "\n",
    "        soup = get_soup(self.url)\n",
    "        table = soup.find_all(\"div\", attrs={\"class\": \"row package-aside\"})\n",
    "\n",
    "        for each in table:\n",
    "            for row in each.find_all(\"a\"):\n",
    "                if row.get(\"title\") == \"Canonical Repository URL\":\n",
    "                    self.canonical_repo = row.text.strip()\n",
    "                if row.text.strip() == \"Homepage\":\n",
    "                    self.home_page = row.get(\"href\")\n",
    "                if row.text.strip() == \"Source\":\n",
    "                    self.source_code = row.get(\"href\")\n",
    "                if row.text.strip() == \"Issues\":\n",
    "                    self.issues = row.get(\"href\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packagist_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"Packagist\"]\n",
    "\n",
    "packagist_missing = packagist_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing Packagist: {packagist_missing.id.nunique()}\")\n",
    "\n",
    "# get links for Packagist Repo\n",
    "packagist_missing[\"links\"] = packagist_missing.apply(\n",
    "    lambda x: PackagistProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "packagist_missing[\"source_code\"] = packagist_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set home page link\n",
    "packagist_missing[\"home_page\"] = packagist_missing.apply(\n",
    "    lambda x: x['links'].home_page,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set canonical_repo link\n",
    "packagist_missing[\"canonical_repo\"] = packagist_missing.apply(\n",
    "    lambda x: x['links'].canonical_repo,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "packagist_missing[\"github_repo\"] = packagist_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else None,\n",
    "    axis=1\n",
    ")\n",
    "# check home_page for github link\n",
    "packagist_missing[\"github_repo\"] = packagist_missing.apply(\n",
    "    lambda x: x['home_page'] if 'github.com/' in str(x['home_page']) else x['github_repo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check home_page for github link\n",
    "packagist_missing[\"github_repo\"] = packagist_missing.apply(\n",
    "    lambda x: x['canonical_repo'] if 'github.com/' in str(x['canonical_repo']) else x['github_repo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# add https:// to link\n",
    "packagist_missing[\"github_repo\"] = packagist_missing.apply(\n",
    "    lambda x: x['github_repo'].replace('github.com', 'https://github.com') if 'github.com/' in str(x['canonical_repo']) else x['github_repo'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for Packagist GHSAs: {packagist_missing[packagist_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for Packagist GHSAs: {packagist_missing[packagist_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "packagist_missing_final = packagist_missing[[\"id\",\"package_ecosystem\",\n",
    "                                             \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "packagist_missing_final = packagist_missing_final[packagist_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {packagist_missing_final.id.nunique()} Packagist GHSAs | ./missing_GH_info/packagist_20230111.csv\")\n",
    "\n",
    "packagist_missing_final.to_csv(f\"./missing_GH_info/packagist_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RubyGems Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(url=url, headers=headers)  # request response from url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "class RubyGemsProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.home_page = None\n",
    "        self.source_code = None\n",
    "        self.issues = None\n",
    "        self.url = f\"https://rubygems.org/gems/{self.package_name}\"\n",
    "\n",
    "        soup = get_soup(self.url)\n",
    "        table = soup.find_all(\"div\", attrs={\"class\": \"gem__aside l-col--r--pad\"})\n",
    "\n",
    "        for each in table:\n",
    "            for row in each.find_all(\"a\"):\n",
    "                if row.text.strip() == \"Homepage\":\n",
    "                    self.home_page = row.get(\"href\")\n",
    "                    if 'github.com/' in str(self.home_page):\n",
    "                        # some link cleaning\n",
    "                        temp_home = self.home_page.split(\"github.com/\")[-1].split('/')[:2]\n",
    "                        self.home_page = f\"https://github.com/{'/'.join(temp_home)}\"\n",
    "                        \n",
    "                if row.text.strip() == \"Source Code\":\n",
    "                    self.source_code = row.get(\"href\")\n",
    "                    if 'github.com/' in str(self.source_code):\n",
    "                        # some link cleaning\n",
    "                        temp_source = self.source_code.split(\"github.com/\")[-1].split('/')[:2]\n",
    "                        self.source_code = f\"https://github.com/{'/'.join(temp_source)}\"\n",
    "                        \n",
    "                if row.text.strip() == \"Bug Tracker\":\n",
    "                    self.issues = row.get(\"href\")\n",
    "                    if 'github.com/' in str(self.issues):\n",
    "                        # some link cleaning\n",
    "                        temp_issues = self.issues.split(\"github.com/\")[-1].split('/')[:2]\n",
    "                        self.issues = f\"https://github.com/{'/'.join(temp_issues)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubygems_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"RubyGems\"]\n",
    "\n",
    "rubygems_missing = rubygems_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing RubyGems: {rubygems_missing.id.nunique()}\")\n",
    "\n",
    "# set links\n",
    "rubygems_missing[\"links\"] = rubygems_missing.apply(\n",
    "    lambda x: RubyGemsProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "rubygems_missing[\"source_code\"] = rubygems_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set home page link\n",
    "rubygems_missing[\"home_page\"] = rubygems_missing.apply(\n",
    "    lambda x: x['links'].home_page,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set issues link\n",
    "rubygems_missing[\"issues\"] = rubygems_missing.apply(\n",
    "    lambda x: x['links'].issues,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check home_page for github link\n",
    "rubygems_missing[\"github_repo\"] = rubygems_missing.apply(\n",
    "    lambda x: x['home_page'] if 'github.com/' in str(x['home_page']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check home_page for github link\n",
    "rubygems_missing[\"github_repo\"] = rubygems_missing.apply(\n",
    "    lambda x: x['issues'] if 'github.com/' in str(x['issues']) else x['github_repo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "rubygems_missing[\"github_repo\"] = rubygems_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else x['github_repo'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for RubyGems GHSAs: {rubygems_missing[rubygems_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for RubyGems GHSAs: {rubygems_missing[rubygems_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "rubygems_missing_final= rubygems_missing[[\"id\",\"package_ecosystem\",\n",
    "                                          \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "rubygems_missing_final = rubygems_missing_final[rubygems_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {rubygems_missing_final.id.nunique()} Packagist GHSAs | ./missing_GH_info/rubygems_20230111.csv\")\n",
    "\n",
    "rubygems_missing_final.to_csv(f\"./missing_GH_info/rubygems_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NuGet Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(url=url, headers=headers)  # request response from url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "class NuGetProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.home_page = None\n",
    "        self.source_code = None\n",
    "        self.url = f\"https://www.nuget.org/packages/{self.package_name}\"\n",
    "\n",
    "        soup = get_soup(self.url)\n",
    "        \n",
    "        if \"Rate limit is exceeded\" in soup.text:\n",
    "            print(soup.text)\n",
    "            print(f\"Sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            # get soup again\n",
    "            soup = get_soup(self.url)\n",
    "            \n",
    "        table = soup.find_all(\"div\", attrs={\"class\": \"sidebar-section\"})\n",
    "        \n",
    "        for each in table:\n",
    "            for row in each.find_all(\"a\"):\n",
    "                if row.text.strip() == \"Project website\":\n",
    "                    self.home_page = row.get(\"href\")\n",
    "                        \n",
    "                if row.text.strip() == \"Source repository\":\n",
    "                    self.source_code = row.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuget_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"NuGet\"]\n",
    "\n",
    "nuget_missing = nuget_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing Nuget: {nuget_missing.id.nunique()}\")\n",
    "\n",
    "# set package links\n",
    "nuget_missing[\"links\"] = nuget_missing.apply(\n",
    "    lambda x: NuGetProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "nuget_missing[\"source_code\"] = nuget_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set home page link\n",
    "nuget_missing[\"home_page\"] = nuget_missing.apply(\n",
    "    lambda x: x['links'].home_page,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check home_page for github link\n",
    "nuget_missing[\"github_repo\"] = nuget_missing.apply(\n",
    "    lambda x: x['home_page'] if 'github.com/' in str(x['home_page']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "nuget_missing[\"github_repo\"] = nuget_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else x['github_repo'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for Nuget GHSAs: {nuget_missing[nuget_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for RubyGems GHSAs: {nuget_missing[nuget_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "nuget_missing_final= nuget_missing[[\"id\",\"package_ecosystem\",\n",
    "                                    \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "nuget_missing_final = nuget_missing_final[nuget_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {nuget_missing_final.id.nunique()} Nuget GHSAs | ./missing_GH_info/nuget_20230111.csv\")\n",
    "\n",
    "nuget_missing_final.to_csv(f\"./missing_GH_info/nuget_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(url=url, headers=headers)  # request response from url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "class GoProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.source_code = None\n",
    "        self.url = f\"https://pkg.go.dev/{self.package_name}\"\n",
    "\n",
    "        soup = get_soup(self.url)\n",
    "            \n",
    "        table = soup.find_all(\"div\", attrs={\"class\": \"UnitMeta-repo\"})\n",
    "        \n",
    "        for each in table:\n",
    "            for row in each.find_all(\"a\"):\n",
    "                self.source_code = row.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"Go\"]\n",
    "\n",
    "go_missing = go_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing Go: {go_missing.id.nunique()}\")\n",
    "\n",
    "# get links for packages\n",
    "go_missing[\"links\"] = go_missing.apply(\n",
    "    lambda x: GoProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "go_missing[\"source_code\"] = go_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "go_missing[\"github_repo\"] = go_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else None,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for Go GHSAs: {go_missing[go_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for Go GHSAs: {go_missing[go_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "go_missing_final= go_missing[[\"id\",\"package_ecosystem\",\n",
    "                                    \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "go_missing_final = go_missing_final[go_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {go_missing_final.id.nunique()} Go GHSAs | ./missing_GH_info/go_20230111.csv\")\n",
    "\n",
    "go_missing_final.to_csv(f\"./missing_GH_info/go_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crates.IO Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CratesProjectLinks():\n",
    "    def __init__(self, package_name):\n",
    "        self.package_name = package_name\n",
    "        self.source_code = None\n",
    "        \n",
    "        # we can use a direct API here\n",
    "        self.url = f\"https://crates.io/api/v1/crates/{self.package_name}\"\n",
    "\n",
    "        response = requests.get(url=self.url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            self.source_code = response.json()[\"crate\"][\"repository\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crates_missing = ghsa_missing_GH_repo[ghsa_missing_GH_repo['package_ecosystem']==\"crates.io\"]\n",
    "\n",
    "crates_missing = crates_missing.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Missing Crates: {crates_missing.id.nunique()}\")\n",
    "\n",
    "crates_missing[\"links\"] = crates_missing.apply(\n",
    "    lambda x: CratesProjectLinks(package_name=x['package_name']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# set source code\n",
    "crates_missing[\"source_code\"] = crates_missing.apply(\n",
    "    lambda x: x['links'].source_code,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if github url in either source_code or home_page\n",
    "crates_missing[\"github_repo\"] = crates_missing.apply(\n",
    "    lambda x: x['source_code'] if 'github.com/' in str(x['source_code']) else None,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found GH Repo for Go GHSAs: {crates_missing[crates_missing['github_repo'].notna()].id.nunique()}\")\n",
    "print(f\"No GH Repo for Go GHSAs: {crates_missing[crates_missing['github_repo'].isna()].id.nunique()}\\n\")\n",
    "\n",
    "# save data to a CSV\n",
    "crates_missing_final= crates_missing[[\"id\",\"package_ecosystem\",\n",
    "                                    \"package_name\", \"github_repo\"]].drop_duplicates()\n",
    "\n",
    "# when saving data only keep the ones we found a GH repo for\n",
    "crates_missing_final = crates_missing_final[crates_missing_final['github_repo'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Saving info for {crates_missing_final.id.nunique()} Crates.io GHSAs | ./missing_GH_info/crates_20230111.csv\")\n",
    "\n",
    "crates_missing_final.to_csv(f\"./missing_GH_info/crates_20230111.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maven_missing_final = pd.read_csv(f\"./missing_GH_info/maven_20230111.csv\")\n",
    "npm_missing_final = pd.read_csv(f\"./missing_GH_info/npm_20230111.csv\")\n",
    "pypi_missing_final = pd.read_csv(f\"./missing_GH_info/pypi_20230111.csv\")\n",
    "packagist_missing_final = pd.read_csv(f\"./missing_GH_info/packagist_20230111.csv\")\n",
    "rubygems_missing_final = pd.read_csv(f\"./missing_GH_info/rubygems_20230111.csv\")\n",
    "go_missing_final = pd.read_csv(f\"./missing_GH_info/go_20230111.csv\")\n",
    "nuget_missing_final = pd.read_csv(f\"./missing_GH_info/nuget_20230111.csv\")\n",
    "crates_missing_final = pd.read_csv(f\"./missing_GH_info/crates_20230111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_missing = maven_missing_final\n",
    "final_missing = pd.concat([final_missing, npm_missing_final])\n",
    "final_missing = pd.concat([final_missing, pypi_missing_final])\n",
    "final_missing = pd.concat([final_missing, packagist_missing_final])\n",
    "final_missing = pd.concat([final_missing, rubygems_missing_final])\n",
    "final_missing = pd.concat([final_missing, go_missing_final])\n",
    "final_missing = pd.concat([final_missing, nuget_missing_final])\n",
    "final_missing = pd.concat([final_missing, crates_missing_final])\n",
    "\n",
    "print(f\"Final Found: {final_missing.id.nunique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a breakdown of ecosystem info\n",
    "breakdowns_missing = ghsa_missing_GH_repo.package_ecosystem.value_counts().to_frame().reset_index(drop=False)\n",
    "breakdowns_missing.columns = [\"ecosystem\", \"source_link_missing_count\"]\n",
    "\n",
    "breakdowns_missing_found = final_missing.package_ecosystem.value_counts().to_frame().reset_index(drop=False)\n",
    "breakdowns_missing_found.columns = [\"ecosystem\", \"source_link_found_count\"]\n",
    "\n",
    "breakdowns_found = pd.merge(breakdowns_missing, breakdowns_missing_found,\n",
    "                      on=[\"ecosystem\"],\n",
    "                      how=\"left\")\n",
    "\n",
    "breakdowns_found = breakdowns_found.fillna(0)\n",
    "\n",
    "breakdowns_found[\"percent_found_source_link\"] = breakdowns_found.apply(\n",
    "      lambda x: f\"{round(100*x['source_link_found_count']/x['source_link_missing_count'], 2)}%\",\n",
    "      axis=1\n",
    ")\n",
    "\n",
    "breakdowns_found[\"missing_found\"] = breakdowns_found.apply(\n",
    "      lambda x: f\"{x['source_link_found_count']} ({x['percent_found_source_link']})\",\n",
    "      axis=1\n",
    ")\n",
    "\n",
    "breakdowns_found.head(n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6669e0aea01684a9d5dc4ed262ee49a72e00b93c6432aeb9e9912f63b574a39e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
